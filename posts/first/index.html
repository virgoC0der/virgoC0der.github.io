<!doctype html><html lang=zh-CN><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>在Mac Mini上部署私有化大语言模型 | Billy's Blog</title>
<link rel=stylesheet href=/css/main.css><script defer src=https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js></script></head><body><div class="prose prose-lg max-w-none p-8 font-sans"><button onclick=history.back() class="mb-6 px-4 py-2 bg-white border-2 border-black font-bold hover:bg-black hover:text-white transition-all shadow-[2px_2px_0px_0px_rgba(0,0,0,1)] active:translate-y-[2px] active:translate-x-[2px] active:shadow-none cursor-pointer">
← 返回</button><h1 class="text-4xl font-black mb-4">在Mac Mini上部署私有化大语言模型</h1><div class="border-b-2 border-black mb-8"></div><h2 id=环境准备>环境准备</h2><ol><li>系统要求：<ul><li>macOS Monterey 12.3 或更高版本</li><li>至少16GB内存（推荐32GB）</li><li>安装Homebrew包管理器</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 安装Homebrew</span>
</span></span><span style=display:flex><span>/bin/bash -c <span style=color:#e6db74>&#34;</span><span style=color:#66d9ef>$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><h2 id=安装部署>安装部署</h2><h3 id=1-安装ollama>1. 安装Ollama</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 通过curl直接安装</span>
</span></span><span style=display:flex><span>curl -fsSL https://ollama.com/install.sh | sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 启动ollama服务</span>
</span></span><span style=display:flex><span>ollama serve
</span></span></code></pre></div><h3 id=2-部署open-webui>2. 部署Open WebUI</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 使用Docker运行webui容器</span>
</span></span><span style=display:flex><span>docker run -d -p 3000:8080 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -v ollama:/root/.ollama <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --name open-webui <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><h2 id=模型加载>模型加载</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 下载llama3模型（根据需求替换模型名称）</span>
</span></span><span style=display:flex><span>ollama pull llama3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看已安装模型</span>
</span></span><span style=display:flex><span>ollama list
</span></span></code></pre></div><h2 id=访问验证>访问验证</h2><ol><li><p>打开浏览器访问：</p><ul><li>Ollama管理界面：http://localhost:11434</li><li>WebUI界面：http://localhost:3000</li></ul></li><li><p>在WebUI界面选择模型即可开始对话</p></li></ol><h2 id=注意事项>注意事项</h2><ol><li>确保3000/11434端口未被占用</li><li>首次下载模型需要较长时间（取决于网络环境）</li><li>建议配合Ngrok实现内网穿透</li><li>使用GPU加速需要额外配置Metal后端</li></ol></div></body></html>